#!/usr/bin/env python3
"""
æµ‹è¯• Langfuse OpenAI Wrapper é›†æˆ

ç›´æ¥æµ‹è¯• OpenAI wrapper æ˜¯å¦èƒ½æ­£ç¡®è¿½è¸ª LLM è°ƒç”¨
"""

import os
from dotenv import load_dotenv

load_dotenv()

def test_openai_wrapper():
    """æµ‹è¯• Langfuse OpenAI wrapper"""
    print("=" * 60)
    print("Langfuse OpenAI Wrapper æµ‹è¯•")
    print("=" * 60)

    # 1. å¯¼å…¥æµ‹è¯•
    print("\n[1/4] å¯¼å…¥æµ‹è¯•...")
    try:
        from langfuse.openai import OpenAI
        print("  âœ… Langfuse OpenAI wrapper å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"  âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False

    # 2. é…ç½®æ£€æŸ¥
    print("\n[2/4] é…ç½®æ£€æŸ¥...")
    config = {
        "LANGFUSE_ENABLED": os.getenv('LANGFUSE_ENABLED', 'false'),
        "LANGFUSE_HOST": os.getenv('LANGFUSE_HOST', ''),
        "LANGFUSE_PUBLIC_KEY": 'å·²è®¾ç½®' if os.getenv('LANGFUSE_PUBLIC_KEY') else 'æœªè®¾ç½®',
        "LANGFUSE_SECRET_KEY": 'å·²è®¾ç½®' if os.getenv('LANGFUSE_SECRET_KEY') else 'æœªè®¾ç½®',
        "LLM_BINDING_HOST": os.getenv('LLM_BINDING_HOST', ''),
        "LLM_MODEL": os.getenv('LLM_MODEL', 'deepseek'),
    }

    for key, value in config.items():
        status = "âœ…" if value and value != 'false' else "âŒ"
        print(f"  {status} {key}: {value}")

    if config["LANGFUSE_ENABLED"].lower() != 'true':
        print("\n  âš ï¸  Langfuse æœªå¯ç”¨ï¼Œä½†ä»å¯æµ‹è¯• wrapper åŠŸèƒ½")

    # 3. åˆ›å»ºå®¢æˆ·ç«¯
    print("\n[3/4] åˆ›å»º Langfuse wrapped OpenAI å®¢æˆ·ç«¯...")
    try:
        client = OpenAI(
            base_url=os.getenv('LLM_BINDING_HOST'),
            api_key=os.getenv('LLM_BINDING_API_KEY')
        )
        print("  âœ… å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        print(f"  â„¹ï¸  Base URL: {os.getenv('LLM_BINDING_HOST')}")
    except Exception as e:
        print(f"  âŒ å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {e}")
        return False

    # 4. æµ‹è¯• LLM è°ƒç”¨
    print("\n[4/4] æµ‹è¯• LLM è°ƒç”¨ï¼ˆå°†è‡ªåŠ¨è¿½è¸ªåˆ° Langfuseï¼‰...")
    try:
        response = client.chat.completions.create(
            model=os.getenv('LLM_MODEL', 'deepseek'),
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": "ç®€å•å›å¤ï¼šæµ‹è¯•æˆåŠŸ"}
            ],
            max_tokens=50
        )

        answer = response.choices[0].message.content
        print(f"  âœ… LLM è°ƒç”¨æˆåŠŸ")
        print(f"  â„¹ï¸  å›ç­”: {answer}")

        if hasattr(response, 'usage') and response.usage:
            print(f"  â„¹ï¸  Token ä½¿ç”¨:")
            print(f"     - Prompt: {response.usage.prompt_tokens}")
            print(f"     - Completion: {response.usage.completion_tokens}")
            print(f"     - Total: {response.usage.total_tokens}")

        print("\n  ğŸ“Š æŸ¥çœ‹è¿½è¸ªæ•°æ®:")
        print(f"     ğŸ‘‰ è®¿é—®: {os.getenv('LANGFUSE_HOST', 'http://localhost:3000')}/traces")
        print(f"     ğŸ‘‰ åº”è¯¥èƒ½çœ‹åˆ°åˆšæ‰çš„ LLM è°ƒç”¨è®°å½•")

        return True

    except Exception as e:
        print(f"  âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("\nğŸ” è¿™ä¸ªæµ‹è¯•å°†:")
    print("   1. å¯¼å…¥ Langfuse OpenAI wrapper")
    print("   2. æ£€æŸ¥é…ç½®")
    print("   3. åˆ›å»º wrapped å®¢æˆ·ç«¯")
    print("   4. å‘é€æµ‹è¯•è¯·æ±‚ï¼ˆè‡ªåŠ¨è¿½è¸ªåˆ° Langfuseï¼‰")
    print()

    success = test_openai_wrapper()

    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ æµ‹è¯•æˆåŠŸï¼")
        print("=" * 60)
        print("\nâœ… Langfuse é›†æˆæ­£å¸¸å·¥ä½œ")
        print("âœ… æ‰€æœ‰ LLM è°ƒç”¨éƒ½ä¼šè‡ªåŠ¨è¿½è¸ª")
        print(f"\nğŸ“Š æŸ¥çœ‹å®Œæ•´è¿½è¸ª: {os.getenv('LANGFUSE_HOST', 'http://localhost:3000')}/traces")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥")
        print("=" * 60)
        print("\nè¯·æ£€æŸ¥:")
        print("  1. Langfuse æœåŠ¡æ˜¯å¦è¿è¡Œ: docker ps | grep langfuse")
        print("  2. .env é…ç½®æ˜¯å¦æ­£ç¡®")
        print("  3. LLM API æ˜¯å¦å¯è®¿é—®")

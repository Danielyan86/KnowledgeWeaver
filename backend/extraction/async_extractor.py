"""
Async Knowledge Graph Extractor
å¼‚æ­¥çŸ¥è¯†å›¾è°±æå–å™¨

æ ¸å¿ƒåŠŸèƒ½ï¼š
- ä½¿ç”¨ Gemini API æˆ–å…¶ä»– LLM API å¹¶å‘è°ƒç”¨
- æ”¯æŒæ–­ç‚¹ç»­ä¼ 
- è‡ªåŠ¨é‡è¯•æœºåˆ¶
- é‡å åˆ†å—å¤„ç†
"""

import asyncio
import json
import re
import os
from typing import Dict, List
from pathlib import Path

from dotenv import load_dotenv
from tqdm.asyncio import tqdm

from ..retrieval.prompts.prompt_loader import get_extraction_prompt, get_document_topic_prompt
from .normalizer import KnowledgeGraphNormalizer
from .entity_filter import get_entity_filter
from ..core.observability import get_tracer


# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class AsyncKnowledgeGraphExtractor:
    """å¼‚æ­¥çŸ¥è¯†å›¾è°±æå–å™¨ï¼ˆæ”¯æŒ Gemini APIï¼‰"""

    def __init__(self):
        """
        åˆå§‹åŒ–å¼‚æ­¥æå–å™¨
        æ‰€æœ‰é…ç½®ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œä¸ä½¿ç”¨é»˜è®¤å‚æ•°
        """
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        self.chunk_size = int(os.getenv('CHUNK_SIZE', '800'))

        # å¹¶å‘æ§åˆ¶
        max_concurrent = int(os.getenv('CONCURRENT_REQUESTS', '5'))
        self.semaphore = asyncio.Semaphore(max_concurrent)

        # LLM åç«¯ï¼šgemini æˆ– openaiï¼ˆæ–‡æ¡£æå–ä¸“ç”¨ï¼‰
        self.backend = os.getenv('EXTRACTION_LLM_BACKEND', 'gemini')

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        if self.backend == 'gemini':
            from google import genai
            self.client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))
            self.model_name = os.getenv('GEMINI_MODEL', 'gemini-2.0-flash-exp')
        else:  # openai å…¼å®¹
            from openai import AsyncOpenAI
            self.client = AsyncOpenAI(
                base_url=os.getenv('LLM_BINDING_HOST'),
                api_key=os.getenv('LLM_BINDING_API_KEY')
            )
            self.model_name = os.getenv('LLM_MODEL')

        # è§„èŒƒåŒ–å™¨
        self.normalizer = KnowledgeGraphNormalizer()

        # Langfuse è¿½è¸ªå™¨
        self.tracer = get_tracer()

        # æ–­ç‚¹ç»­ä¼ ç›®å½•
        checkpoint_dir = os.getenv('CHECKPOINT_DIR')
        if checkpoint_dir is None:
            checkpoint_dir = Path(__file__).parent.parent / "data" / "checkpoints"
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

    def chunk_text_overlapped(self, text: str, overlap: float = None) -> List[str]:
        """
        é‡å åˆ†å—ï¼ˆé¿å…è¾¹ç•Œä¸¢å¤±ä¿¡æ¯ï¼‰

        Args:
            text: åŸå§‹æ–‡æœ¬
            overlap: é‡å æ¯”ä¾‹ï¼Œä»ç¯å¢ƒå˜é‡è¯»å–

        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        if overlap is None:
            overlap = float(os.getenv('CHUNK_OVERLAP_RATIO', '0.5'))

        if not text:
            return []

        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text)

        chunks = []
        current_chunk = ""
        overlap_buffer = ""

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šå·²æœ‰å†…å®¹è¶…è¿‡é™åˆ¶ï¼Œå…ˆä¿å­˜å½“å‰å—
            if len(current_chunk) + len(para) > self.chunk_size and current_chunk:
                chunks.append(current_chunk.strip())

                # ä¿ç•™é‡å éƒ¨åˆ†
                overlap_size = int(len(current_chunk) * overlap)
                overlap_buffer = current_chunk[-overlap_size:]
                current_chunk = overlap_buffer + para + "\n\n"
            else:
                current_chunk += para + "\n\n"

        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return chunks

    def _parse_llm_response(self, response_text: str) -> Dict:
        """
        è§£æ LLM å“åº”ï¼Œæå– JSON

        Args:
            response_text: LLM çš„åŸå§‹å“åº”

        Returns:
            è§£æåçš„å­—å…¸ï¼ŒåŒ…å« entities å’Œ relations
        """
        # å°è¯•æ‰¾åˆ° JSON ä»£ç å—
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response_text)
        if json_match:
            json_str = json_match.group(1)
        else:
            # å°è¯•ç›´æ¥è§£æ
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                json_str = json_match.group(0)
            else:
                return {"entities": [], "relations": []}

        try:
            result = json.loads(json_str)

            # åº”ç”¨å®ä½“è¿‡æ»¤å™¨ï¼Œæå‡å›¾è°±è´¨é‡
            entity_filter = get_entity_filter()
            filtered_result = entity_filter.filter_graph({
                "entities": result.get("entities", []),
                "relations": result.get("relations", [])
            })

            return filtered_result
        except json.JSONDecodeError:
            return {"entities": [], "relations": []}

    async def _call_llm(self, prompt: str) -> str:
        """
        è°ƒç”¨ LLMï¼ˆæ”¯æŒ Gemini å’Œ OpenAI å…¼å®¹ APIï¼‰

        è‡ªåŠ¨è¿½è¸ªåˆ° Langfuse

        Args:
            prompt: å®Œæ•´æç¤ºæ–‡æœ¬

        Returns:
            LLM çš„å“åº”
        """
        import time

        start_time = time.time()

        try:
            if self.backend == 'gemini':
                # Gemini APIï¼ˆåŒæ­¥è°ƒç”¨ï¼Œéœ€è¦åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: self.client.models.generate_content(
                        model=self.model_name,
                        contents=prompt
                    )
                )
                output_text = response.text

                # è®°å½• Gemini è°ƒç”¨ä¿¡æ¯ï¼ˆæ—¥å¿—å½¢å¼ï¼‰
                if self.tracer.enabled:
                    latency_ms = (time.time() - start_time) * 1000
                    print(f"ğŸ“Š [Gemini API] æ¨¡å‹: {self.model_name}, å»¶è¿Ÿ: {round(latency_ms)}ms, "
                          f"è¾“å…¥: {len(prompt)} å­—ç¬¦, è¾“å‡º: {len(output_text)} å­—ç¬¦")

                    # æ³¨æ„ï¼šç”±äº Langfuse v2/v3 API å…¼å®¹æ€§é—®é¢˜ï¼Œ
                    # Gemini è°ƒç”¨æš‚æ—¶åªè®°å½•åˆ°æœåŠ¡å™¨æ—¥å¿—ï¼Œä¸è®°å½•åˆ° Dashboard
                    # OpenAI å…¼å®¹ API çš„è°ƒç”¨ä»ç„¶ä¼šæ­£å¸¸è¿½è¸ªåˆ° Langfuse

                return output_text
            else:
                # OpenAI å…¼å®¹ APIï¼ˆå·²è¢« wrapper è‡ªåŠ¨è¿½è¸ªï¼‰
                response = await self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±æå–ä¸“å®¶ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.1,
                    max_tokens=2000
                )
                return response.choices[0].message.content

        except Exception as e:
            raise Exception(f"LLM è°ƒç”¨å¤±è´¥: {e}")

    async def extract_chunk_bounded(self, chunk: str, chunk_id: int, context: str = "") -> Dict:
        """
        å¸¦é™æµçš„å•ä¸ªå—æå–ï¼ˆå¼‚æ­¥ï¼‰

        Args:
            chunk: æ–‡æœ¬å—
            chunk_id: å— ID
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            æå–ç»“æœ
        """
        async with self.semaphore:
            # æ„å»ºæç¤º
            prompt = get_extraction_prompt(context + chunk if context else chunk)

            # é‡è¯•æœºåˆ¶ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
            max_retries = int(os.getenv('MAX_RETRIES', '3'))
            for attempt in range(max_retries):
                try:
                    # è°ƒç”¨ LLM
                    response_text = await self._call_llm(prompt)

                    # è§£æå“åº”
                    result = self._parse_llm_response(response_text)

                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    self._save_checkpoint(chunk_id, result)
                    return result

                except Exception as e:
                    if attempt == max_retries - 1:
                        print(f"å— {chunk_id} å¤„ç†å¤±è´¥: {e}")
                        return {"entities": [], "relations": []}

                    # æŒ‡æ•°é€€é¿
                    wait_time = (2 ** attempt)
                    await asyncio.sleep(wait_time)

    def _save_checkpoint(self, chunk_id: int, result: Dict):
        """ä¿å­˜å•ä¸ªå—çš„æ£€æŸ¥ç‚¹"""
        checkpoint_file = self.checkpoint_dir / f"chunk_{chunk_id}.json"
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False)

    def _load_checkpoints(self, total_chunks: int) -> List[Dict]:
        """åŠ è½½å·²å®Œæˆçš„æ£€æŸ¥ç‚¹"""
        results = [None] * total_chunks
        for i in range(total_chunks):
            checkpoint_file = self.checkpoint_dir / f"chunk_{i}.json"
            if checkpoint_file.exists():
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    results[i] = json.load(f)
        return results

    def _clear_checkpoints(self):
        """æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        for checkpoint_file in self.checkpoint_dir.glob("chunk_*.json"):
            checkpoint_file.unlink()

    async def _extract_document_topic(self, text: str) -> str:
        """
        æå–æ–‡æ¡£ä¸»é¢˜

        Args:
            text: æ–‡æ¡£æ–‡æœ¬ï¼ˆå–å‰1000å­—ï¼‰

        Returns:
            æ–‡æ¡£ä¸»é¢˜æè¿°
        """
        sample = text[:1000] if len(text) > 1000 else text

        try:
            prompt = get_document_topic_prompt(sample)
            response = await self._call_llm(prompt)
            return response.strip()
        except Exception:
            return ""

    def merge_graphs(self, graphs: List[Dict], connect_islands: bool = True) -> Dict:
        """
        åˆå¹¶å¤šä¸ªæå–ç»“æœ

        Args:
            graphs: å¤šä¸ªæå–ç»“æœåˆ—è¡¨
            connect_islands: æ˜¯å¦å°è¯•è¿æ¥å­¤å²›

        Returns:
            åˆå¹¶åçš„å›¾è°±æ•°æ®
        """
        all_entities = {}  # name -> entity
        all_relations = []
        seen_relations = set()

        for graph in graphs:
            # åˆå¹¶å®ä½“
            for entity in graph.get("entities", []):
                name = entity.get("name", "").strip()
                if not name:
                    continue

                if name not in all_entities:
                    all_entities[name] = {
                        "name": name,
                        "type": entity.get("type", "Entity"),
                        "description": entity.get("description", "")
                    }
                else:
                    # åˆå¹¶æè¿°
                    existing = all_entities[name]
                    if entity.get("description") and not existing.get("description"):
                        existing["description"] = entity.get("description")

            # åˆå¹¶å…³ç³»
            for relation in graph.get("relations", []):
                source = relation.get("source", "").strip()
                target = relation.get("target", "").strip()
                rel = relation.get("relation", "").strip()

                if not source or not target or not rel:
                    continue

                # å»é‡
                rel_key = f"{source}-{rel}->{target}"
                if rel_key not in seen_relations:
                    seen_relations.add(rel_key)
                    all_relations.append({
                        "source": source,
                        "relation": rel,
                        "target": target
                    })

        return {
            "entities": list(all_entities.values()),
            "relations": all_relations
        }

    def _convert_to_graph_format(self, extracted: Dict) -> Dict:
        """
        å°† LLM æå–çš„æ•°æ®è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼

        Args:
            extracted: LLM æå–çš„åŸå§‹æ•°æ®

        Returns:
            å‰ç«¯æ ¼å¼çš„å›¾è°±æ•°æ®
        """
        # æ„å»ºèŠ‚ç‚¹
        nodes = []
        node_ids = set()

        for entity in extracted.get("entities", []):
            name = entity.get("name", "").strip()
            if not name or name in node_ids:
                continue

            node_ids.add(name)
            nodes.append({
                "id": name,
                "label": name,
                "type": entity.get("type", "Entity"),
                "description": entity.get("description", ""),
                "properties": {},
                "degree": 0
            })

        # æ„å»ºè¾¹
        edges = []
        for relation in extracted.get("relations", []):
            source = relation.get("source", "").strip()
            target = relation.get("target", "").strip()
            rel = relation.get("relation", "").strip()

            if not source or not target or source == target:
                continue

            # ç¡®ä¿æºèŠ‚ç‚¹å­˜åœ¨
            if source not in node_ids:
                node_ids.add(source)
                nodes.append({
                    "id": source,
                    "label": source,
                    "type": "Entity",
                    "description": "",
                    "properties": {},
                    "degree": 0
                })

            # ç¡®ä¿ç›®æ ‡èŠ‚ç‚¹å­˜åœ¨
            if target not in node_ids:
                node_ids.add(target)
                nodes.append({
                    "id": target,
                    "label": target,
                    "type": "Entity",
                    "description": "",
                    "properties": {},
                    "degree": 0
                })

            edges.append({
                "source": source,
                "target": target,
                "label": rel,
                "weight": 1
            })

        # è®¡ç®—èŠ‚ç‚¹åº¦æ•°
        degree_map = {}
        for edge in edges:
            degree_map[edge["source"]] = degree_map.get(edge["source"], 0) + 1
            degree_map[edge["target"]] = degree_map.get(edge["target"], 0) + 1

        for node in nodes:
            node["degree"] = degree_map.get(node["id"], 0)

        return {"nodes": nodes, "edges": edges}

    async def extract_document_async(self, file_path: str, resume: bool = True,
                                     return_chunks: bool = False,
                                     progress_callback: callable = None,
                                     cancellation_check: callable = None) -> Dict:
        """
        å¼‚æ­¥æ–‡æ¡£å¤„ç†ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œä¸­æ–­ï¼‰

        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            resume: æ˜¯å¦ä»æ–­ç‚¹ç»­ä¼ 
            return_chunks: æ˜¯å¦è¿”å›åŸå§‹ chunksï¼ˆç”¨äº RAG ç´¢å¼•ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(current, total, stage)
            cancellation_check: ä¸­æ–­æ£€æŸ¥å‡½æ•° cancellation_check() -> bool

        Returns:
            æå–å¹¶è§„èŒƒåŒ–åçš„å›¾è°±æ•°æ®

        Raises:
            Exception: å¦‚æœå¤„ç†è¢«ä¸­æ–­
        """
        path = Path(file_path)

        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        # è¯»å–æ–‡ä»¶å†…å®¹
        if path.suffix.lower() == '.txt':
            with open(path, 'r', encoding='utf-8') as f:
                text = f.read()
        elif path.suffix.lower() == '.pdf':
            raise NotImplementedError("PDF è§£æå°šæœªå®ç°")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}")

        # æå–æ–‡æ¡£ä¸»é¢˜
        doc_topic = await self._extract_document_topic(text)
        if doc_topic:
            print(f"æ–‡æ¡£ä¸»é¢˜: {doc_topic}")

        # åˆ†å—ï¼ˆé‡å ï¼‰
        chunks = self.chunk_text_overlapped(text)
        total = len(chunks)
        print(f"æ–‡æ¡£åˆ†æˆ {total} ä¸ªå—ï¼ˆé‡å åˆ†å—ï¼‰")

        # åˆå§‹åŒ–è¿›åº¦
        if progress_callback:
            progress_callback(0, total, "åˆ†å—å®Œæˆ")

        # åŠ è½½å·²å®Œæˆçš„æ£€æŸ¥ç‚¹
        results = self._load_checkpoints(total) if resume else [None] * total
        completed = sum(1 for r in results if r is not None)
        print(f"å·²å®Œæˆ {completed}/{total} å—ï¼Œç»§ç»­å¤„ç†å‰©ä½™éƒ¨åˆ†...")

        # æ›´æ–°è¿›åº¦ï¼ˆå·²å®Œæˆçš„æ£€æŸ¥ç‚¹ï¼‰
        if progress_callback and completed > 0:
            progress_callback(completed, total, "æ¢å¤æ–­ç‚¹")

        # æ”¶é›†æ ¸å¿ƒå®ä½“ä½œä¸ºä¸Šä¸‹æ–‡
        core_entities = set()
        for result in results:
            if result:
                for entity in result.get("entities", []):
                    name = entity.get("name", "")
                    if name and len(name) <= 10:
                        core_entities.add(name)

        # åˆ›å»ºä»»åŠ¡ï¼ˆåªå¤„ç†æœªå®Œæˆçš„å—ï¼‰
        tasks = []
        for i, chunk in enumerate(chunks):
            if results[i] is None:
                # æ„å»ºä¸Šä¸‹æ–‡
                context = ""
                if doc_topic:
                    context += f"æ–‡æ¡£èƒŒæ™¯ï¼š{doc_topic}\n"
                if core_entities:
                    context += f"å·²è¯†åˆ«çš„æ ¸å¿ƒå®ä½“ï¼š{', '.join(list(core_entities)[:10])}\n"
                    context += "è¯·æ³¨æ„ï¼šå¦‚æœå½“å‰æ–‡æœ¬ä¸è¿™äº›å®ä½“ç›¸å…³ï¼Œè¯·å»ºç«‹å…³ç³»è¿æ¥ã€‚\n\n"

                tasks.append((i, self.extract_chunk_bounded(chunk, i, context)))

        # å¹¶å‘æ‰§è¡Œï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        if tasks:
            print(f"å¼€å§‹å¼‚æ­¥å¤„ç† {len(tasks)} ä¸ªæœªå®Œæˆçš„å—...")
            for idx, (i, task_coro) in enumerate(tqdm(tasks, desc="æå–çŸ¥è¯†å›¾è°±")):
                # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
                if cancellation_check and cancellation_check():
                    print(f"\nâš ï¸  å¤„ç†è¢«ç”¨æˆ·ä¸­æ–­ (å·²å®Œæˆ {completed + idx}/{total} å—)")
                    raise Exception("å¤„ç†è¢«ç”¨æˆ·ä¸­æ–­")

                result = await task_coro
                results[i] = result

                # æ›´æ–°æ ¸å¿ƒå®ä½“
                for entity in result.get("entities", []):
                    name = entity.get("name", "")
                    if name and len(name) <= 10:
                        core_entities.add(name)

                # æ›´æ–°è¿›åº¦
                if progress_callback:
                    current_completed = completed + idx + 1
                    progress_callback(current_completed, total, "æå–å®ä½“å’Œå…³ç³»")

        # æ›´æ–°è¿›åº¦ï¼šåˆå¹¶
        if progress_callback:
            progress_callback(total, total, "åˆå¹¶ç»“æœ")

        # åˆå¹¶ç»“æœ
        merged = self.merge_graphs([r for r in results if r])
        print(f"åˆå¹¶åï¼š{len(merged['entities'])} ä¸ªå®ä½“ï¼Œ{len(merged['relations'])} ä¸ªå…³ç³»")

        # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼
        graph_data = self._convert_to_graph_format(merged)

        # è§„èŒƒåŒ–
        normalized = self.normalizer.normalize_graph(graph_data)
        print(f"è§„èŒƒåŒ–åï¼š{normalized['stats']}")

        # æ¸…ç†æ£€æŸ¥ç‚¹
        self._clear_checkpoints()

        # å¦‚æœéœ€è¦è¿”å› chunks ç”¨äº RAG ç´¢å¼•
        if return_chunks:
            normalized["chunks"] = chunks
            normalized["doc_topic"] = doc_topic

        return normalized


# å‘½ä»¤è¡Œæµ‹è¯•
if __name__ == "__main__":
    import sys

    async def main():
        extractor = AsyncKnowledgeGraphExtractor()

        if len(sys.argv) > 1:
            # å¤„ç†æ–‡ä»¶
            file_path = sys.argv[1]
            result = await extractor.extract_document_async(file_path)
            print(json.dumps(result, ensure_ascii=False, indent=2))
        else:
            # æµ‹è¯•æ–‡æœ¬
            test_text = """
            æç¬‘æ¥åœ¨ã€Šè®©æ—¶é—´é™ªä½ æ…¢æ…¢å˜å¯Œã€‹ä¸­ä¸»å¼ å®šæŠ•ç­–ç•¥ã€‚
            ä»–è®¤ä¸ºå®šæŠ•æ˜¯æœ€é€‚åˆæ™®é€šäººçš„æŠ•èµ„æ–¹å¼ï¼Œæ¨èæ ‡æ™®500æŒ‡æ•°åŸºé‡‘ã€‚
            é•¿æœŸä¸»ä¹‰æ˜¯è¿™æœ¬ä¹¦çš„æ ¸å¿ƒç†å¿µï¼Œå¼ºè°ƒæ—¶é—´å¤åˆ©çš„é‡è¦æ€§ã€‚
            """

            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            import tempfile
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
                f.write(test_text)
                temp_path = f.name

            result = await extractor.extract_document_async(temp_path)
            print(json.dumps(result, ensure_ascii=False, indent=2))

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            Path(temp_path).unlink()

    asyncio.run(main())

# Langfuse é›†æˆæœ€ä½³å®è·µ

## ğŸ¯ æ ¸å¿ƒåŸåˆ™ï¼šæœ€å°åŒ–ä»£ç å…¥ä¾µ

å¥½çš„å¯è§‚æµ‹æ€§é›†æˆåº”è¯¥ï¼š
- âœ… **å¯¹ä¸šåŠ¡ä»£ç å½±å“æœ€å°**
- âœ… **æ˜“äºå¯ç”¨/ç¦ç”¨**
- âœ… **æ— éœ€æ”¹å˜å‡½æ•°ç­¾å**
- âœ… **è‡ªåŠ¨è¿½è¸ªï¼Œæ— éœ€æ‰‹åŠ¨è°ƒç”¨**

## âœ… æ¨èæ–¹å¼ï¼šOpenAI Wrapperï¼ˆæˆ‘ä»¬ä½¿ç”¨çš„æ–¹æ¡ˆï¼‰

### åŸç†
Langfuse æä¾›äº† OpenAI SDK çš„åŒ…è£…å™¨ï¼Œå¯ä»¥**è‡ªåŠ¨æ‹¦æˆªæ‰€æœ‰ LLM è°ƒç”¨**ï¼Œæ— éœ€ä¿®æ”¹ä¸šåŠ¡ä»£ç ã€‚

### å®ç°ï¼ˆ3æ­¥ï¼‰

**æ­¥éª¤ 1: é…ç½® Langfuse**
```python
# backend/core/observability.py
from langfuse import Langfuse

class LangfuseTracer:
    def __init__(self):
        self.client = Langfuse(
            public_key=os.getenv('LANGFUSE_PUBLIC_KEY'),
            secret_key=os.getenv('LANGFUSE_SECRET_KEY'),
            host=os.getenv('LANGFUSE_HOST')
        )

    def wrap_openai(self, client):
        """åŒ…è£… OpenAI å®¢æˆ·ç«¯"""
        from langfuse.openai import OpenAI as LangfuseOpenAI
        return LangfuseOpenAI(
            base_url=client.base_url,
            api_key=client.api_key
        )
```

**æ­¥éª¤ 2: åŒ…è£… OpenAI å®¢æˆ·ç«¯**
```python
# backend/retrieval/qa_engine.py
class QAEngine:
    def __init__(self):
        # åˆ›å»ºæ™®é€šå®¢æˆ·ç«¯
        client = OpenAI(base_url=api_base, api_key=api_key)

        # ç”¨ Langfuse wrapper åŒ…è£…ï¼ˆä¸€è¡Œä»£ç ï¼ï¼‰
        tracer = get_tracer()
        self.client = tracer.wrap_openai(client)  # âœ… ä»…æ­¤ä¸€è¡Œ!

    def _generate_answer(self, prompt: str) -> str:
        # ä¸šåŠ¡ä»£ç å®Œå…¨ä¸å˜ï¼
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
        )
        return response.choices[0].message.content
```

**æ­¥éª¤ 3: é…ç½®ç¯å¢ƒå˜é‡å¯ç”¨/ç¦ç”¨**
```bash
# .env
LANGFUSE_ENABLED=true  # ç¦ç”¨åªéœ€æ”¹ä¸º false
LANGFUSE_PUBLIC_KEY=pk-xxx
LANGFUSE_SECRET_KEY=sk-xxx
```

### ä¼˜åŠ¿å¯¹æ¯”

| æ–¹å¼ | ä»£ç ä¿®æ”¹ | å‡½æ•°ç­¾åå˜åŒ– | æ˜“äºç¦ç”¨ | è‡ªåŠ¨è¿½è¸ª Token |
|------|---------|-------------|---------|--------------|
| **OpenAI Wrapper** âœ… | 1è¡Œ | æ—  | âœ… | âœ… |
| æ‰‹åŠ¨ trace | æ¯ä¸ªå‡½æ•° 10+ è¡Œ | éœ€æ·»åŠ  trace_id | âŒ | âŒ |
| è£…é¥°å™¨ | æ¯ä¸ªå‡½æ•° 1 è¡Œ | æ—  | âœ… | âŒ |

## âŒ ä¸æ¨èï¼šå…¥ä¾µå¼æ‰‹åŠ¨è¿½è¸ª

```python
# âŒ ä¸å¥½ï¼šéœ€è¦ä¿®æ”¹å‡½æ•°ç­¾å
def ask(question: str, trace_id: str = None):
    if trace_id:
        tracer.start_trace(trace_id)
    # ... ä¸šåŠ¡é€»è¾‘
    if trace_id:
        tracer.end_trace(trace_id)

# âŒ ä¸å¥½ï¼šéœ€è¦æ‰‹åŠ¨è®°å½• token
def _generate_answer(prompt, trace_id):
    response = client.chat.completions.create(...)
    if trace_id:
        tracer.log_llm_call(
            trace_id=trace_id,
            prompt_tokens=response.usage.prompt_tokens,  # æ‰‹åŠ¨è·å–
            completion_tokens=response.usage.completion_tokens,
            # ... æ›´å¤šå­—æ®µ
        )
```

**é—®é¢˜**ï¼š
- æ¯ä¸ªå‡½æ•°éƒ½è¦æ·»åŠ  trace_id å‚æ•°
- éœ€è¦æ‰‹åŠ¨ä¼ é€’ trace_id åˆ°æ‰€æœ‰è°ƒç”¨é“¾
- éœ€è¦æ‰‹åŠ¨è®°å½• token å’Œæˆæœ¬
- ç¦ç”¨è¿½è¸ªéœ€è¦æ³¨é‡Šå¤§é‡ä»£ç 
- å®¹æ˜“é—æ¼æŸäº›è°ƒç”¨

## ğŸ”§ å…¶ä»–é›†æˆæ–¹å¼å¯¹æ¯”

### æ–¹å¼ 1: è£…é¥°å™¨ï¼ˆé€‚ç”¨äºç‰¹å®šå‡½æ•°è¿½è¸ªï¼‰

```python
from langfuse.decorators import observe

class QAEngine:
    @observe()  # è¿½è¸ªæ•´ä¸ªå‡½æ•°æ‰§è¡Œ
    def ask(self, question: str):
        # ä¸šåŠ¡ä»£ç ä¸å˜
        response = self.retriever.retrieve(question)
        answer = self._generate_answer(response)
        return answer
```

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… éœ€è¦è¿½è¸ªç‰¹å®šä¸šåŠ¡æµç¨‹ï¼ˆå¦‚æ£€ç´¢ã€RAG pipelineï¼‰
- âœ… éœ€è¦è‡ªå®šä¹‰è¿½è¸ªåç§°å’Œå…ƒæ•°æ®
- âŒ ä¸é€‚åˆè¿½è¸ª LLM è°ƒç”¨ï¼ˆtoken ä¸ä¼šè‡ªåŠ¨è®°å½•ï¼‰

### æ–¹å¼ 2: LangChain é›†æˆï¼ˆå¦‚æœä½¿ç”¨ LangChainï¼‰

```python
from langfuse.callback import CallbackHandler

# å®Œå…¨éå…¥ä¾µï¼
langfuse_handler = CallbackHandler()

# åœ¨è°ƒç”¨æ—¶ä¼ å…¥
chain.invoke(
    {"question": "..."},
    config={"callbacks": [langfuse_handler]}
)
```

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… é¡¹ç›®ä½¿ç”¨ LangChain/LlamaIndex
- âœ… æƒ³è¦è¿½è¸ªæ•´ä¸ª chain æ‰§è¡Œ
- âŒ ä½ çš„é¡¹ç›®ä¸ç”¨ LangChainï¼Œæ‰€ä»¥ä¸é€‚ç”¨

### æ–¹å¼ 3: Context Managerï¼ˆé€‚ç”¨äºä»£ç å—è¿½è¸ªï¼‰

```python
from langfuse import Langfuse

langfuse = Langfuse()

with langfuse.trace(name="document_processing") as trace:
    # è¿™ä¸ªä»£ç å—å†…çš„æ‰€æœ‰æ“ä½œéƒ½ä¼šè¢«è¿½è¸ª
    extract_entities()
    build_graph()
    save_to_neo4j()
```

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… éœ€è¦è¿½è¸ªä¸€æ®µå¤æ‚æµç¨‹
- âœ… éœ€è¦åµŒå¥—è¿½è¸ªï¼ˆtrace é‡Œé¢æœ‰ spanï¼‰
- âŒ ä¸é€‚åˆç®€å•çš„ API è°ƒç”¨è¿½è¸ª

## ğŸ“Š æˆ‘ä»¬çš„é›†æˆæ–¹æ¡ˆæ€»ç»“

### å½“å‰å®ç°ï¼ˆæ¨èï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä¸šåŠ¡ä»£ç  (qa_engine.py)            â”‚
â”‚                                      â”‚
â”‚  client.chat.completions.create()   â”‚
â”‚         â†“                            â”‚
â”‚  Langfuse OpenAI Wrapper (è‡ªåŠ¨æ‹¦æˆª) â”‚
â”‚         â†“                            â”‚
â”‚  è®°å½•åˆ° Langfuse (token/æˆæœ¬/å»¶è¿Ÿ)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä»£ç æ”¹åŠ¨é‡**ï¼š
- âœ… `observability.py`: 50 è¡Œï¼ˆå¯å¤ç”¨ï¼‰
- âœ… `qa_engine.py`: **1 è¡Œ**ï¼ˆåŒ…è£…å®¢æˆ·ç«¯ï¼‰
- âœ… å…¶ä»–æ–‡ä»¶: 0 è¡Œ

**åŠŸèƒ½**ï¼š
- âœ… è‡ªåŠ¨è¿½è¸ªæ‰€æœ‰ LLM è°ƒç”¨
- âœ… è‡ªåŠ¨è®°å½• token ä½¿ç”¨é‡
- âœ… è‡ªåŠ¨è®¡ç®—æˆæœ¬
- âœ… è‡ªåŠ¨è®°å½•å»¶è¿Ÿ
- âœ… ç¯å¢ƒå˜é‡æ§åˆ¶å¯ç”¨/ç¦ç”¨

### æœªæ¥æ‰©å±•ï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦è¿½è¸ªæ›´å¤šå†…å®¹ï¼ˆå¦‚æ£€ç´¢ã€æ–‡æ¡£å¤„ç†ï¼‰ï¼Œå¯ä»¥æ·»åŠ ï¼š

```python
# æ–¹æ¡ˆ A: è£…é¥°å™¨ï¼ˆæ¨èï¼‰
@observe()
def retrieve(self, question: str):
    # ... æ£€ç´¢é€»è¾‘

# æ–¹æ¡ˆ B: Context Manager
with langfuse.trace(name="extract_document"):
    # ... æå–é€»è¾‘
```

## ğŸ“ å­¦ä¹ èµ„æº

- [Langfuse OpenAI Integration](https://langfuse.com/docs/integrations/openai)
- [Langfuse Decorators](https://langfuse.com/docs/sdk/python/decorators)
- [Best Practices](https://langfuse.com/docs/tracing)

## ğŸ’¡ å¸¸è§é—®é¢˜

### Q: æˆ‘å¿…é¡»æ”¹æ‰€æœ‰ä»£ç å—ï¼Ÿ
**A**: ä¸ï¼åªéœ€åŒ…è£… OpenAI å®¢æˆ·ç«¯ï¼ˆ1 è¡Œä»£ç ï¼‰ï¼Œæ‰€æœ‰è°ƒç”¨è‡ªåŠ¨è¿½è¸ªã€‚

### Q: å¦‚ä½•ç¦ç”¨è¿½è¸ªï¼Ÿ
**A**: è®¾ç½®ç¯å¢ƒå˜é‡ `LANGFUSE_ENABLED=false`ï¼Œæ— éœ€æ”¹ä»£ç ã€‚

### Q: ä¼šå½±å“æ€§èƒ½å—ï¼Ÿ
**A**: è¿½è¸ªæ•°æ®æ˜¯å¼‚æ­¥å‘é€çš„ï¼Œå¯¹ API å“åº”æ—¶é—´å½±å“ <10msã€‚

### Q: æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹ endpoint å—ï¼Ÿ
**A**: æ”¯æŒï¼åªè¦å…¼å®¹ OpenAI API æ ¼å¼å³å¯ï¼ˆæˆ‘ä»¬çš„é¡¹ç›®å°±æ˜¯ï¼‰ã€‚

### Q: èƒ½è¿½è¸ª Gemini/Claude è°ƒç”¨å—ï¼Ÿ
**A**: å¯ä»¥ï¼Œä½†éœ€è¦ä½¿ç”¨è£…é¥°å™¨æˆ–æ‰‹åŠ¨è¿½è¸ªï¼ˆæš‚æ— è‡ªåŠ¨ wrapperï¼‰ã€‚

## âœ… æ€»ç»“

**æœ€ä½³å®è·µé‡‘å­—å¡”**ï¼ˆä»ç®€å•åˆ°å¤æ‚ï¼‰ï¼š

```
Level 1: OpenAI Wrapperï¼ˆå½“å‰å®ç°ï¼‰     â† æ¨èèµ·ç‚¹ï¼Œ90% åœºæ™¯å¤Ÿç”¨
         â†“ 1 è¡Œä»£ç ï¼Œè‡ªåŠ¨è¿½è¸ªæ‰€æœ‰ LLM è°ƒç”¨

Level 2: è£…é¥°å™¨ @observe                â† éœ€è¦è¿½è¸ªç‰¹å®šä¸šåŠ¡æµç¨‹æ—¶
         â†“ æ¯ä¸ªå‡½æ•° 1 è¡Œï¼Œè¿½è¸ªæ•´ä¸ªæµç¨‹

Level 3: Context Manager                â† éœ€è¦è¿½è¸ªå¤æ‚åµŒå¥—æµç¨‹æ—¶
         â†“ ä»£ç å—çº§åˆ«çš„ç»†ç²’åº¦æ§åˆ¶

Level 4: æ‰‹åŠ¨ API è°ƒç”¨                  â† ä»…åœ¨ç‰¹æ®Šéœ€æ±‚æ—¶ä½¿ç”¨
         â†“ å®Œå…¨è‡ªå®šä¹‰ï¼Œä½†ä»£ç é‡å¤§
```

**æˆ‘ä»¬é€‰æ‹© Level 1**ï¼Œå› ä¸ºï¼š
- âœ… ä»£ç æ”¹åŠ¨æœ€å°ï¼ˆ1 è¡Œï¼‰
- âœ… è¿½è¸ªè¦†ç›–æœ€å…¨ï¼ˆæ‰€æœ‰ LLM è°ƒç”¨ï¼‰
- âœ… æœ€å®¹æ˜“ç»´æŠ¤
- âœ… æ»¡è¶³ 90% çš„ç›‘æ§éœ€æ±‚

**ä½•æ—¶å‡çº§åˆ° Level 2/3**ï¼š
- éœ€è¦è¿½è¸ªæ£€ç´¢æ€§èƒ½
- éœ€è¦è¿½è¸ªæ–‡æ¡£å¤„ç†æµç¨‹
- éœ€è¦è‡ªå®šä¹‰å…ƒæ•°æ®
